{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection using BorutaShap\n",
    "from sklearn.base import clone\n",
    "\n",
    "model_BS = clone(model)\n",
    "\n",
    "Feature_Selector = BorutaShap(model=model_BS, \n",
    "                              importance_measure='shap', \n",
    "                              classification=True, \n",
    "                              percentile=100, \n",
    "                              pvalue=0.05)\n",
    "\n",
    "Feature_Selector.fit(X=X_train_features, \n",
    "                     y=y_train, \n",
    "                     n_trials=100, \n",
    "                     sample=False, \n",
    "                     train_or_test = 'train', \n",
    "                     normalize=True, \n",
    "                     verbose=False, \n",
    "                     random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boruta plot\n",
    "\n",
    "Feature_Selector.plot(X_size=15,\n",
    "                      figsize=(15,45),\n",
    "                      which_features='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionality reduction using feature importance\n",
    "\n",
    "df_imp = pd.DataFrame({'imp':model.feature_importances_}, index = model.get_booster().feature_names)\n",
    "df_imp = df_imp[df_imp.imp > 0].sort_values('imp').copy()\n",
    "\n",
    "feat_num = df_imp.shape[0]\n",
    "print(\"total number of features =\", feat_num)\n",
    "df_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_list = [mean(scores)]\n",
    "for i in range(0,feat_num+1):\n",
    "  features = df_imp.index[i:(feat_num+1)].to_list()\n",
    "  mark_feature = X_train_features.columns[X_train_features.columns.isin(features)]\n",
    "  X_train_new = X_train_features[mark_feature].copy()\n",
    "  model_adj = XGBClassifier(colsample_bytree=0.6, gamma=0.8, learning_rate=0.01, max_depth=3, \n",
    "                          min_child_weight=6, n_estimators=500, objective=\"binary:logistic\",\n",
    "                          reg_alpha=0.5, reg_lambda=2, scale_pos_weight=14, subsample=0.2, \n",
    "                          nthread=6, random_state=SEED)\n",
    "  scores_adj = evaluate_model(X_train_new, y_train, model_adj)\n",
    "  print('%d, AUC = %.3f (%.3f)' % (i, mean(scores_adj), std(scores_adj)))\n",
    "  auc_list.append(mean(scores_adj))\n",
    "  #if (max(auc_list)-mean(scores_adj)) >= 0.005:\n",
    "  #  break\n",
    "  #else:\n",
    "  #  auc_list.append(mean(scores_adj))\n",
    "\n",
    "# plot\n",
    "plt.plot(auc_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

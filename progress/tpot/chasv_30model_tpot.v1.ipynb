{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chasv_30model_tpot.v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danssa/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    }
   ],
   "source": [
    "SEED = 0\n",
    "cores = 16\n",
    "from numpy.random import seed\n",
    "seed(SEED)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.preprocessing import KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-d31144c6a883>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['eGFR_ab30'] = np.where(df2['eGFR_ckd']<30,1,0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eGFR_ab</th>\n",
       "      <th>eGFR_ckd</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>he_uph</th>\n",
       "      <th>he_unitr</th>\n",
       "      <th>he_usg</th>\n",
       "      <th>he_upro</th>\n",
       "      <th>he_uglu</th>\n",
       "      <th>he_uket</th>\n",
       "      <th>he_ubil</th>\n",
       "      <th>he_ubld</th>\n",
       "      <th>he_uro</th>\n",
       "      <th>leucocyte</th>\n",
       "      <th>dm</th>\n",
       "      <th>htn</th>\n",
       "      <th>eGFR_ab30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.067598</td>\n",
       "      <td>93.719380</td>\n",
       "      <td>0.428929</td>\n",
       "      <td>47.179829</td>\n",
       "      <td>6.183206</td>\n",
       "      <td>0.020280</td>\n",
       "      <td>1.018628</td>\n",
       "      <td>0.331024</td>\n",
       "      <td>0.234011</td>\n",
       "      <td>0.205590</td>\n",
       "      <td>0.039356</td>\n",
       "      <td>0.757068</td>\n",
       "      <td>0.168417</td>\n",
       "      <td>0.516135</td>\n",
       "      <td>0.025325</td>\n",
       "      <td>0.039046</td>\n",
       "      <td>0.014049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.251056</td>\n",
       "      <td>22.147902</td>\n",
       "      <td>0.494924</td>\n",
       "      <td>15.570474</td>\n",
       "      <td>0.824044</td>\n",
       "      <td>0.140957</td>\n",
       "      <td>0.007915</td>\n",
       "      <td>0.769946</td>\n",
       "      <td>0.888690</td>\n",
       "      <td>0.669646</td>\n",
       "      <td>0.324305</td>\n",
       "      <td>1.252923</td>\n",
       "      <td>0.560018</td>\n",
       "      <td>1.024153</td>\n",
       "      <td>0.157111</td>\n",
       "      <td>0.193706</td>\n",
       "      <td>0.117692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.704754</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.005000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>80.803010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>95.813740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>110.264000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.025000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>257.176000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             eGFR_ab       eGFR_ckd           male            age  \\\n",
       "count  220020.000000  220020.000000  220020.000000  220020.000000   \n",
       "mean        0.067598      93.719380       0.428929      47.179829   \n",
       "std         0.251056      22.147902       0.494924      15.570474   \n",
       "min         0.000000       1.704754       0.000000      18.000000   \n",
       "25%         0.000000      80.803010       0.000000      35.000000   \n",
       "50%         0.000000      95.813740       0.000000      45.000000   \n",
       "75%         0.000000     110.264000       1.000000      58.000000   \n",
       "max         1.000000     257.176000       1.000000      95.000000   \n",
       "\n",
       "              he_uph       he_unitr         he_usg        he_upro  \\\n",
       "count  220020.000000  220020.000000  220020.000000  220020.000000   \n",
       "mean        6.183206       0.020280       1.018628       0.331024   \n",
       "std         0.824044       0.140957       0.007915       0.769946   \n",
       "min         5.000000       0.000000       1.005000       0.000000   \n",
       "25%         5.500000       0.000000       1.010000       0.000000   \n",
       "50%         6.000000       0.000000       1.020000       0.000000   \n",
       "75%         7.000000       0.000000       1.025000       0.000000   \n",
       "max         9.000000       1.000000       1.030000       5.000000   \n",
       "\n",
       "             he_uglu        he_uket        he_ubil        he_ubld  \\\n",
       "count  220020.000000  220020.000000  220020.000000  220020.000000   \n",
       "mean        0.234011       0.205590       0.039356       0.757068   \n",
       "std         0.888690       0.669646       0.324305       1.252923   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       1.000000   \n",
       "max         5.000000       5.000000       4.000000       5.000000   \n",
       "\n",
       "              he_uro      leucocyte             dm            htn  \\\n",
       "count  220020.000000  220020.000000  220020.000000  220020.000000   \n",
       "mean        0.168417       0.516135       0.025325       0.039046   \n",
       "std         0.560018       1.024153       0.157111       0.193706   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       1.000000       0.000000       0.000000   \n",
       "max         5.000000       4.000000       1.000000       1.000000   \n",
       "\n",
       "           eGFR_ab30  \n",
       "count  220020.000000  \n",
       "mean        0.014049  \n",
       "std         0.117692  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/danssa/proj_ua/data/chasv_development.v1.csv\", dtype={'id':np.str})\n",
    "df2 = df.loc[df['from']!=\"knhanes\"]\n",
    "df2['eGFR_ab30'] = np.where(df2['eGFR_ckd']<30,1,0)\n",
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edges :  [18.         49.94635961 68.85955582 95.        ]\n",
      "age0_edge: 49.94635960905048 \n",
      "age1_edge: 68.85955582356871 \n",
      "age2_edge: 95.0\n",
      "age group:\n",
      " 0     466\n",
      "1    1118\n",
      "2    1507\n",
      "Name: level, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#3group age split  \n",
    "\n",
    "##step 1 finding edge value\n",
    "abnormal_disc = df2.query('eGFR_ab30==1').loc[:,'age']\n",
    "abnormal_disc = pd.DataFrame(abnormal_disc)\n",
    "\n",
    "est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='kmeans')\n",
    "est.fit(abnormal_disc)\n",
    "\n",
    "ab_disc = est.transform(abnormal_disc).astype('float')\n",
    "print(\"edges : \", est.bin_edges_[0])\n",
    "\n",
    "age0_edge = est.bin_edges_[0][1]\n",
    "age1_edge = est.bin_edges_[0][2]\n",
    "age2_edge = est.bin_edges_[0][3]\n",
    "print('age0_edge:', age0_edge, '\\nage1_edge:', age1_edge, '\\nage2_edge:', age2_edge)\n",
    "\n",
    "abnormal_disc['level'] = abnormal_disc.apply(lambda x : 0 if x['age']<age0_edge else 1 if x['age']<age1_edge else 2, axis=1)\n",
    "print('age group:\\n',abnormal_disc['level'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    131133\n",
      "1     63825\n",
      "2     25062\n",
      "Name: level, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "##make 3group by age\n",
    "\n",
    "df3 = df2.copy()\n",
    "\n",
    "df3['level'] = df3.apply(lambda x : 0 if x['age']<age0_edge else 1 if x['age']<age1_edge else 2, axis=1)\n",
    "print(df3['level'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total cases = 131133\n",
      "total abnormal function of kidney = 466\n",
      "train0 : 373 test0 : 93\n"
     ]
    }
   ],
   "source": [
    "##age0 group\n",
    "X_age0 = df3[df3['level']==0]\n",
    "y_age0 = X_age0['eGFR_ab30'].astype(\"int64\")\n",
    "\n",
    "print(\"total cases = %d\" %X_age0.shape[0])\n",
    "print(\"total abnormal function of kidney = %d\" %sum(y_age0))\n",
    "\n",
    "X_train0, X_test0, y_train0, y_test0 = train_test_split(X_age0, y_age0, test_size=0.2, stratify=y_age0, random_state=SEED)\n",
    "print(\"train0 : %d\" % sum(y_train0), \"test0 : %d\" % sum(y_test0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total cases = 63825\n",
      "total abnormal function of kidney = 1118\n",
      "train1 : 894 test0 : 224\n"
     ]
    }
   ],
   "source": [
    "##age1 group\n",
    "X_age1 = df3[df3['level']==1]\n",
    "y_age1 = X_age1['eGFR_ab30']\n",
    "\n",
    "print(\"total cases = %d\" %X_age1.shape[0])\n",
    "print(\"total abnormal function of kidney = %d\" %sum(y_age1))\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_age1, y_age1, test_size=0.2, stratify=y_age1, random_state=SEED)\n",
    "print(\"train1 : %d\" % sum(y_train1), \"test0 : %d\" % sum(y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total cases = 25062\n",
      "total abnormal function of kidney = 1507\n",
      "train2 : 1206 test2 : 301\n"
     ]
    }
   ],
   "source": [
    "##age2 group\n",
    "X_age2 = df3[df3['level']==2]\n",
    "y_age2 = X_age2['eGFR_ab30']\n",
    "\n",
    "print(\"total cases = %d\" %X_age2.shape[0])\n",
    "print(\"total abnormal function of kidney = %d\" %sum(y_age2))\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_age2, y_age2, test_size=0.2, stratify=y_age2, random_state=SEED)\n",
    "print(\"train2 : %d\" % sum(y_train2), \"test2 : %d\" % sum(y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total cases = 220020\n",
      "total abnormal function of kidney = 3091\n"
     ]
    }
   ],
   "source": [
    "##concat both trainset and testset\n",
    "X_train = pd.concat([X_train0, X_train1, X_train2])\n",
    "y_train = pd.concat([y_train0, y_train1, y_train2])\n",
    "\n",
    "X_test = pd.concat([X_test0, X_test1, X_test2])\n",
    "y_test = pd.concat([y_test0, y_test1, y_test2])\n",
    "\n",
    "print(\"total cases = %d\" % (X_train.shape[0] + X_test.shape[0]))\n",
    "print(\"total abnormal function of kidney = %d\" % (sum(y_train) + sum(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176015 train cases, 12 variables\n",
      "44005 test cases\n"
     ]
    }
   ],
   "source": [
    "X_train_features = X_train.loc[:, 'male':'leucocyte']\n",
    "\n",
    "print('%d train cases, %d variables' % (X_train_features.shape[0], X_train_features.shape[1]))\n",
    "print('%d test cases'%X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>he_uph</th>\n",
       "      <th>he_unitr</th>\n",
       "      <th>he_usg</th>\n",
       "      <th>he_upro</th>\n",
       "      <th>he_uglu</th>\n",
       "      <th>he_uket</th>\n",
       "      <th>he_ubil</th>\n",
       "      <th>he_ubld</th>\n",
       "      <th>he_uro</th>\n",
       "      <th>leucocyte</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>176015.000000</td>\n",
       "      <td>1.760150e+05</td>\n",
       "      <td>1.760150e+05</td>\n",
       "      <td>176015.000000</td>\n",
       "      <td>1.760150e+05</td>\n",
       "      <td>176015.000000</td>\n",
       "      <td>176015.000000</td>\n",
       "      <td>176015.000000</td>\n",
       "      <td>176015.000000</td>\n",
       "      <td>176015.000000</td>\n",
       "      <td>176015.000000</td>\n",
       "      <td>176015.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.429350</td>\n",
       "      <td>2.221872e-16</td>\n",
       "      <td>-3.972241e-16</td>\n",
       "      <td>0.020209</td>\n",
       "      <td>1.692918e-14</td>\n",
       "      <td>0.331227</td>\n",
       "      <td>0.233327</td>\n",
       "      <td>0.205130</td>\n",
       "      <td>0.040014</td>\n",
       "      <td>0.758543</td>\n",
       "      <td>0.167741</td>\n",
       "      <td>0.514490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.494985</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>0.140713</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>0.770470</td>\n",
       "      <td>0.887225</td>\n",
       "      <td>0.669495</td>\n",
       "      <td>0.326671</td>\n",
       "      <td>1.254166</td>\n",
       "      <td>0.559464</td>\n",
       "      <td>1.022325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.874471e+00</td>\n",
       "      <td>-1.435462e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.723133e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.822702e-01</td>\n",
       "      <td>-8.282717e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.091191e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.397992e-01</td>\n",
       "      <td>-2.210813e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.726920e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.954129e-01</td>\n",
       "      <td>9.932995e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.046336e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.072555e+00</td>\n",
       "      <td>3.422061e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.436575e+00</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                male           age        he_uph       he_unitr        he_usg  \\\n",
       "count  176015.000000  1.760150e+05  1.760150e+05  176015.000000  1.760150e+05   \n",
       "mean        0.429350  2.221872e-16 -3.972241e-16       0.020209  1.692918e-14   \n",
       "std         0.494985  1.000003e+00  1.000003e+00       0.140713  1.000003e+00   \n",
       "min         0.000000 -1.874471e+00 -1.435462e+00       0.000000 -1.723133e+00   \n",
       "25%         0.000000 -7.822702e-01 -8.282717e-01       0.000000 -1.091191e+00   \n",
       "50%         0.000000 -1.397992e-01 -2.210813e-01       0.000000  1.726920e-01   \n",
       "75%         1.000000  6.954129e-01  9.932995e-01       0.000000  8.046336e-01   \n",
       "max         1.000000  3.072555e+00  3.422061e+00       1.000000  1.436575e+00   \n",
       "\n",
       "             he_upro        he_uglu        he_uket        he_ubil  \\\n",
       "count  176015.000000  176015.000000  176015.000000  176015.000000   \n",
       "mean        0.331227       0.233327       0.205130       0.040014   \n",
       "std         0.770470       0.887225       0.669495       0.326671   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         5.000000       5.000000       5.000000       4.000000   \n",
       "\n",
       "             he_ubld         he_uro      leucocyte  \n",
       "count  176015.000000  176015.000000  176015.000000  \n",
       "mean        0.758543       0.167741       0.514490  \n",
       "std         1.254166       0.559464       1.022325  \n",
       "min         0.000000       0.000000       0.000000  \n",
       "25%         0.000000       0.000000       0.000000  \n",
       "50%         0.000000       0.000000       0.000000  \n",
       "75%         1.000000       0.000000       1.000000  \n",
       "max         5.000000       5.000000       4.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#standardization\n",
    "\n",
    "scaler = StandardScaler()\n",
    "std_cols=['age','he_uph','he_usg']\n",
    "std_df=X_train_features[std_cols]\n",
    "\n",
    "X_train_features[std_cols]=scaler.fit_transform(std_df)\n",
    "X_train_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(y_train)\n",
    "estimate = round(counter[0]/counter[1])\n",
    "step = round((estimate - 1)/3)\n",
    "estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danssa/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/distributed/node.py:151: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 40975 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#https://dask-cuda.readthedocs.io/en/latest/quickstart.html\n",
    "\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Create a Dask Cluster with one worker per GPU\n",
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 0.11.6.post3 of tpot is outdated. Version 0.11.7 was released Wednesday January 06, 2021.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fb6a9f4ff24b28b8e27958a6555003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/10100 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9621710504340133\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.8, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=2, XGBClassifier__min_child_weight=11, XGBClassifier__n_estimators=1000, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.75, XGBClassifier__reg_lambda=6, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.6000000000000001, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9624437128476349\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.4000000000000001, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=2, XGBClassifier__min_child_weight=6, XGBClassifier__n_estimators=750, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=2, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9624437128476349\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.4000000000000001, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=2, XGBClassifier__min_child_weight=6, XGBClassifier__n_estimators=750, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=2, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9624437128476349\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.4000000000000001, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=2, XGBClassifier__min_child_weight=6, XGBClassifier__n_estimators=750, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=2, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9624437128476349\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.4000000000000001, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=2, XGBClassifier__min_child_weight=6, XGBClassifier__n_estimators=750, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=2, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9624437128476349\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.4000000000000001, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=2, XGBClassifier__min_child_weight=6, XGBClassifier__n_estimators=750, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=2, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9624607401358712\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=15, XGBClassifier__n_estimators=750, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=6, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9624607401358712\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=15, XGBClassifier__n_estimators=750, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=6, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9624607401358712\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=15, XGBClassifier__n_estimators=750, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=6, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9624671887920628\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=0.8, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=14, XGBClassifier__n_estimators=750, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=4, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9624986535974003\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.4000000000000001, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=2, XGBClassifier__min_child_weight=10, XGBClassifier__n_estimators=750, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=2, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9627910503946151\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=19, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=2, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628158734921504\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=19, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.75, XGBClassifier__reg_lambda=2, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628158734921504\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=19, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.75, XGBClassifier__reg_lambda=2, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628158734921504\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=19, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.75, XGBClassifier__reg_lambda=2, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628158734921504\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=19, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.75, XGBClassifier__reg_lambda=2, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628158734921504\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=19, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.75, XGBClassifier__reg_lambda=2, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628158734921504\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=19, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.75, XGBClassifier__reg_lambda=2, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628158734921504\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=19, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.75, XGBClassifier__reg_lambda=2, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628457345451815\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=19, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628457345451815\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=19, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.962868031885354\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=19, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=2, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628777994484559\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.6, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=19, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=2, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628777994484559\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.6, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=19, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=2, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628777994484559\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.6, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=19, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=2, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628777994484559\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.6, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=19, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=2, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628780040178289\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.2000000000000002, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=19, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=2, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628780040178289\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.2000000000000002, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=19, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=2, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628780040178289\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.2000000000000002, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=19, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=2, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 51 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 52 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 53 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 54 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 55 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 56 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 57 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 58 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 59 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 60 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 61 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 62 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 63 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 64 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 65 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 66 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 67 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 68 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 69 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 70 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 71 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 72 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 73 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 74 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 75 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 76 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 77 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 78 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 79 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 80 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 81 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 82 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 83 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 84 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 85 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 86 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 87 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 88 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 89 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 90 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 91 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 92 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9628916257349791\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=1.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=4, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.25, XGBClassifier__reg_lambda=1, XGBClassifier__scale_pos_weight=70, XGBClassifier__subsample=0.4, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[19:19:10] /opt/conda/envs/rapids/conda-bld/xgboost_1607619219243/work/src/c_api/../data/../common/device_helpers.cuh:400: Memory allocation error on worker 0: std::bad_alloc: CUDA error at: /home/danssa/anaconda3/envs/rapids-0.17/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory\n- Free memory: 5832704\n- Requested memory: 1408120\n\nStack trace:\n  [bt] (0) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(+0x14eb6f) [0x7fa63f0dcb6f]\n  [bt] (1) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(dh::detail::ThrowOOMError(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned long)+0x3ad) [0x7fa63f3164bd]\n  [bt] (2) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(dh::detail::XGBDefaultDeviceAllocatorImpl<xgboost::detail::GradientPairInternal<float> >::allocate(unsigned long)+0x152) [0x7fa63f355d22]\n  [bt] (3) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(thrust::detail::vector_base<xgboost::detail::GradientPairInternal<float>, dh::detail::XGBDefaultDeviceAllocatorImpl<xgboost::detail::GradientPairInternal<float> > >::append(unsigned long)+0x1b0) [0x7fa63f359630]\n  [bt] (4) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >::SetDevice(int) const+0x87) [0x7fa63f365eb7]\n  [bt] (5) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x390) [0x7fa63f47b3a0]\n  [bt] (6) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(+0x2517d7) [0x7fa63f1df7d7]\n  [bt] (7) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fa63f0e3728]\n  [bt] (8) /home/danssa/anaconda3/envs/rapids-0.17/lib/python3.8/lib-dynload/../../libffi.so.7(+0x69ed) [0x7faab8e869ed]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/tpot/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 self._pop, _ = eaMuPlusLambda(\n\u001b[0m\u001b[1;32m    731\u001b[0m                     \u001b[0mpopulation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/tpot/gp_deap.py\u001b[0m in \u001b[0;36meaMuPlusLambda\u001b[0;34m(population, toolbox, mu, lambda_, cxpb, mutpb, ngen, pbar, stats, halloffame, verbose, per_generation_function, log_file)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0moffspring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffspring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/tpot/base.py\u001b[0m in \u001b[0;36m_evaluate_individuals\u001b[0;34m(self, population, features, target, sample_weight, groups)\u001b[0m\n\u001b[1;32m   1364\u001b[0m                             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m                             \u001b[0mtmp_result_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtmp_result_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, dsk, keys, workers, allow_other_workers, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\u001b[0m\n\u001b[1;32m   2654\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2655\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2656\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[1;32m   1963\u001b[0m                 \u001b[0mlocal_worker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m             return self.sync(\n\u001b[0m\u001b[1;32m   1965\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/distributed/client.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m             return sync(\n\u001b[0m\u001b[1;32m    839\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/tornado/gen.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m                         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/distributed/client.py\u001b[0m in \u001b[0;36m_gather\u001b[0;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[1;32m   1828\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1829\u001b[0;31m                             \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1830\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/dask_ml/model_selection/methods.py\u001b[0m in \u001b[0;36mcreate_cv_results\u001b[0;34m()\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mfit_times\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_times\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-8accc91eee80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mtraining_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mtpot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mtpot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/danssa/proj_ua/paper_cha/CHA+SV/30model/chasv_30model.v1.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/tpot/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[1;32m    771\u001b[0m                     \u001b[0;31m# raise the exception if it's our last attempt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattempts\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/tpot/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_top_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_summary_of_best_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m                     \u001b[0;31m# Delete the temporary cache before exiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cleanup_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/tpot/base.py\u001b[0m in \u001b[0;36m_summary_of_best_pipeline\u001b[0;34m(self, features, target)\u001b[0m\n\u001b[1;32m    888\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitted_pipeline_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbosity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m    907\u001b[0m             eval_group=None, label_transform=label_transform)\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m         self._Booster = train(xgb_options, train_dmatrix,\n\u001b[0m\u001b[1;32m    910\u001b[0m                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_boosting_rounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                               \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mBooster\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \"\"\"\n\u001b[0;32m--> 219\u001b[0;31m     bst = _train_internal(params, dtrain,\n\u001b[0m\u001b[1;32m    220\u001b[0m                           \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                           \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[1;32m   1268\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \"\"\"\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [19:19:10] /opt/conda/envs/rapids/conda-bld/xgboost_1607619219243/work/src/c_api/../data/../common/device_helpers.cuh:400: Memory allocation error on worker 0: std::bad_alloc: CUDA error at: /home/danssa/anaconda3/envs/rapids-0.17/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory\n- Free memory: 5832704\n- Requested memory: 1408120\n\nStack trace:\n  [bt] (0) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(+0x14eb6f) [0x7fa63f0dcb6f]\n  [bt] (1) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(dh::detail::ThrowOOMError(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned long)+0x3ad) [0x7fa63f3164bd]\n  [bt] (2) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(dh::detail::XGBDefaultDeviceAllocatorImpl<xgboost::detail::GradientPairInternal<float> >::allocate(unsigned long)+0x152) [0x7fa63f355d22]\n  [bt] (3) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(thrust::detail::vector_base<xgboost::detail::GradientPairInternal<float>, dh::detail::XGBDefaultDeviceAllocatorImpl<xgboost::detail::GradientPairInternal<float> > >::append(unsigned long)+0x1b0) [0x7fa63f359630]\n  [bt] (4) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >::SetDevice(int) const+0x87) [0x7fa63f365eb7]\n  [bt] (5) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x390) [0x7fa63f47b3a0]\n  [bt] (6) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(+0x2517d7) [0x7fa63f1df7d7]\n  [bt] (7) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fa63f0e3728]\n  [bt] (8) /home/danssa/anaconda3/envs/rapids-0.17/lib/python3.8/lib-dynload/../../libffi.so.7(+0x69ed) [0x7faab8e869ed]\n\n"
     ]
    }
   ],
   "source": [
    "classifier_config_dict = {\n",
    "\n",
    "    # xgboost tree method = gpu hist\n",
    "    \n",
    "    'xgboost.XGBClassifier': {\n",
    "        'n_estimators': [100, 250 ,500, 750, 1000],\n",
    "        'learning_rate': [1e-2, 1e-1, 0.3],\n",
    "        'max_depth': range(2, 11),\n",
    "        'min_child_weight': range(1, 21),\n",
    "        'gamma':np.arange(0, 2.01, 0.2),\n",
    "        'subsample': np.arange(0.2, 1.01, 0.2),\n",
    "        'colsample_bytree': np.arange(0.4,1.01,0.2),\n",
    "        \"reg_alpha\": [0, 0.25, 0.5, 0.75, 1],\n",
    "        \"reg_lambda\": [1, 2, 4, 6, 8],\n",
    "        'scale_pos_weight': [estimate],\n",
    "        'objective': ['binary:logistic'],\n",
    "        'tree_method' : ['gpu_hist'],\n",
    "        'n_jobs': [1],\n",
    "        'verbosity': [0]\n",
    "    },\n",
    "\n",
    "}\n",
    "\n",
    "tpot = TPOTClassifier(scoring=\"roc_auc\",\n",
    "                      cv=5,\n",
    "                      random_state=SEED,\n",
    "                      n_jobs=4,\n",
    "                      verbosity=3,\n",
    "                      generations=100,\n",
    "                      population_size=100,\n",
    "                      use_dask=True,\n",
    "                      warm_start=False,\n",
    "                      config_dict=classifier_config_dict,\n",
    "                      template='Classifier')\n",
    "\n",
    "training_features=X_train_features.copy(deep=True)\n",
    "tpot.fit(training_features, y_train)\n",
    "\n",
    "tpot.export('/home/danssa/proj_ua/paper_cha/CHA+SV/30model/chasv_30model.v1.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.export('/home/danssa/proj_ua/paper_cha/CHA+SV/30model/chasv_30model.v1.py')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi_gpu",
   "language": "python",
   "name": "rapids-0.17"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

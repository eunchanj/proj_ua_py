{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chasv_60model_tpot.v4.1\n",
    "split 9:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danssa/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    }
   ],
   "source": [
    "SEED = 0\n",
    "cores = 16\n",
    "from numpy.random import seed\n",
    "seed(SEED)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.preprocessing import KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eGFR_ab</th>\n",
       "      <th>eGFR_ckd</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>he_uph</th>\n",
       "      <th>he_unitr</th>\n",
       "      <th>he_usg</th>\n",
       "      <th>he_upro</th>\n",
       "      <th>he_uglu</th>\n",
       "      <th>he_uket</th>\n",
       "      <th>he_ubil</th>\n",
       "      <th>he_ubld</th>\n",
       "      <th>he_uro</th>\n",
       "      <th>leucocyte</th>\n",
       "      <th>dm</th>\n",
       "      <th>htn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "      <td>220020.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.067598</td>\n",
       "      <td>93.719380</td>\n",
       "      <td>0.428929</td>\n",
       "      <td>47.179829</td>\n",
       "      <td>6.183206</td>\n",
       "      <td>0.020280</td>\n",
       "      <td>1.018628</td>\n",
       "      <td>0.331024</td>\n",
       "      <td>0.234011</td>\n",
       "      <td>0.205590</td>\n",
       "      <td>0.039356</td>\n",
       "      <td>0.757068</td>\n",
       "      <td>0.168417</td>\n",
       "      <td>0.516135</td>\n",
       "      <td>0.025325</td>\n",
       "      <td>0.039046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.251056</td>\n",
       "      <td>22.147902</td>\n",
       "      <td>0.494924</td>\n",
       "      <td>15.570474</td>\n",
       "      <td>0.824044</td>\n",
       "      <td>0.140957</td>\n",
       "      <td>0.007915</td>\n",
       "      <td>0.769946</td>\n",
       "      <td>0.888690</td>\n",
       "      <td>0.669646</td>\n",
       "      <td>0.324305</td>\n",
       "      <td>1.252923</td>\n",
       "      <td>0.560018</td>\n",
       "      <td>1.024153</td>\n",
       "      <td>0.157111</td>\n",
       "      <td>0.193706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.704754</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.005000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>80.803010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>95.813740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>110.264000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.025000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>257.176000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             eGFR_ab       eGFR_ckd           male            age  \\\n",
       "count  220020.000000  220020.000000  220020.000000  220020.000000   \n",
       "mean        0.067598      93.719380       0.428929      47.179829   \n",
       "std         0.251056      22.147902       0.494924      15.570474   \n",
       "min         0.000000       1.704754       0.000000      18.000000   \n",
       "25%         0.000000      80.803010       0.000000      35.000000   \n",
       "50%         0.000000      95.813740       0.000000      45.000000   \n",
       "75%         0.000000     110.264000       1.000000      58.000000   \n",
       "max         1.000000     257.176000       1.000000      95.000000   \n",
       "\n",
       "              he_uph       he_unitr         he_usg        he_upro  \\\n",
       "count  220020.000000  220020.000000  220020.000000  220020.000000   \n",
       "mean        6.183206       0.020280       1.018628       0.331024   \n",
       "std         0.824044       0.140957       0.007915       0.769946   \n",
       "min         5.000000       0.000000       1.005000       0.000000   \n",
       "25%         5.500000       0.000000       1.010000       0.000000   \n",
       "50%         6.000000       0.000000       1.020000       0.000000   \n",
       "75%         7.000000       0.000000       1.025000       0.000000   \n",
       "max         9.000000       1.000000       1.030000       5.000000   \n",
       "\n",
       "             he_uglu        he_uket        he_ubil        he_ubld  \\\n",
       "count  220020.000000  220020.000000  220020.000000  220020.000000   \n",
       "mean        0.234011       0.205590       0.039356       0.757068   \n",
       "std         0.888690       0.669646       0.324305       1.252923   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       1.000000   \n",
       "max         5.000000       5.000000       4.000000       5.000000   \n",
       "\n",
       "              he_uro      leucocyte             dm            htn  \n",
       "count  220020.000000  220020.000000  220020.000000  220020.000000  \n",
       "mean        0.168417       0.516135       0.025325       0.039046  \n",
       "std         0.560018       1.024153       0.157111       0.193706  \n",
       "min         0.000000       0.000000       0.000000       0.000000  \n",
       "25%         0.000000       0.000000       0.000000       0.000000  \n",
       "50%         0.000000       0.000000       0.000000       0.000000  \n",
       "75%         0.000000       1.000000       0.000000       0.000000  \n",
       "max         5.000000       4.000000       1.000000       1.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/danssa/proj_ua/data/chasv_development.v1.csv\", dtype={'id':np.str})\n",
    "df2 = df.loc[df['from']!=\"knhanes\"]\n",
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edges :  [18.         53.80721514 70.94886739 95.        ]\n",
      "age0_edge: 53.80721513971053 \n",
      "age1_edge: 70.94886739427912 \n",
      "age2_edge: 95.0\n",
      "age group:\n",
      " 0    2104\n",
      "1    5505\n",
      "2    7264\n",
      "Name: level, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#3group age split  \n",
    "\n",
    "##step 1 finding edge value\n",
    "abnormal_disc = df2.query('eGFR_ab==1').loc[:,'age']\n",
    "abnormal_disc = pd.DataFrame(abnormal_disc)\n",
    "\n",
    "est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='kmeans')\n",
    "est.fit(abnormal_disc)\n",
    "\n",
    "ab_disc = est.transform(abnormal_disc).astype('float')\n",
    "print(\"edges : \", est.bin_edges_[0])\n",
    "\n",
    "age0_edge = est.bin_edges_[0][1]\n",
    "age1_edge = est.bin_edges_[0][2]\n",
    "age2_edge = est.bin_edges_[0][3]\n",
    "print('age0_edge:', age0_edge, '\\nage1_edge:', age1_edge, '\\nage2_edge:', age2_edge)\n",
    "\n",
    "abnormal_disc['level'] = abnormal_disc.apply(lambda x : 0 if x['age']<age0_edge else 1 if x['age']<age1_edge else 2, axis=1)\n",
    "print('age group:\\n',abnormal_disc['level'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    147140\n",
      "1     52377\n",
      "2     20503\n",
      "Name: level, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "##make 3group by age\n",
    "\n",
    "df3 = df2.copy()\n",
    "\n",
    "df3['level'] = df3.apply(lambda x : 0 if x['age']<age0_edge else 1 if x['age']<age1_edge else 2, axis=1)\n",
    "print(df3['level'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total cases = 147140\n",
      "total abnormal function of kidney = 2104\n",
      "train0 : 1894 test0 : 210\n"
     ]
    }
   ],
   "source": [
    "##age0 group\n",
    "X_age0 = df3[df3['level']==0]\n",
    "y_age0 = X_age0['eGFR_ab'].astype(\"int64\")\n",
    "\n",
    "print(\"total cases = %d\" %X_age0.shape[0])\n",
    "print(\"total abnormal function of kidney = %d\" %sum(y_age0))\n",
    "\n",
    "X_train0, X_test0, y_train0, y_test0 = train_test_split(X_age0, y_age0, test_size=0.1, stratify=y_age0, random_state=SEED)\n",
    "print(\"train0 : %d\" % sum(y_train0), \"test0 : %d\" % sum(y_test0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total cases = 52377\n",
      "total abnormal function of kidney = 5505\n",
      "train1 : 4954 test0 : 551\n"
     ]
    }
   ],
   "source": [
    "##age1 group\n",
    "X_age1 = df3[df3['level']==1]\n",
    "y_age1 = X_age1['eGFR_ab']\n",
    "\n",
    "print(\"total cases = %d\" %X_age1.shape[0])\n",
    "print(\"total abnormal function of kidney = %d\" %sum(y_age1))\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_age1, y_age1, test_size=0.1, stratify=y_age1, random_state=SEED)\n",
    "print(\"train1 : %d\" % sum(y_train1), \"test0 : %d\" % sum(y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total cases = 20503\n",
      "total abnormal function of kidney = 7264\n",
      "train2 : 6537 test2 : 727\n"
     ]
    }
   ],
   "source": [
    "##age2 group\n",
    "X_age2 = df3[df3['level']==2]\n",
    "y_age2 = X_age2['eGFR_ab']\n",
    "\n",
    "print(\"total cases = %d\" %X_age2.shape[0])\n",
    "print(\"total abnormal function of kidney = %d\" %sum(y_age2))\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_age2, y_age2, test_size=0.1, stratify=y_age2, random_state=SEED)\n",
    "print(\"train2 : %d\" % sum(y_train2), \"test2 : %d\" % sum(y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total cases = 220020\n",
      "total abnormal function of kidney = 14873\n"
     ]
    }
   ],
   "source": [
    "##concat both trainset and testset\n",
    "X_train = pd.concat([X_train0, X_train1, X_train2])\n",
    "y_train = pd.concat([y_train0, y_train1, y_train2])\n",
    "\n",
    "X_test = pd.concat([X_test0, X_test1, X_test2])\n",
    "y_test = pd.concat([y_test0, y_test1, y_test2])\n",
    "\n",
    "print(\"total cases = %d\" % (X_train.shape[0] + X_test.shape[0]))\n",
    "print(\"total abnormal function of kidney = %d\" % (sum(y_train) + sum(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198017 train cases, 12 variables\n",
      "22003 test cases\n"
     ]
    }
   ],
   "source": [
    "X_train_features = X_train.loc[:, 'male':'leucocyte']\n",
    "\n",
    "print('%d train cases, %d variables' % (X_train_features.shape[0], X_train_features.shape[1]))\n",
    "print('%d test cases'%X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>he_uph</th>\n",
       "      <th>he_unitr</th>\n",
       "      <th>he_usg</th>\n",
       "      <th>he_upro</th>\n",
       "      <th>he_uglu</th>\n",
       "      <th>he_uket</th>\n",
       "      <th>he_ubil</th>\n",
       "      <th>he_ubld</th>\n",
       "      <th>he_uro</th>\n",
       "      <th>leucocyte</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>198017.000000</td>\n",
       "      <td>1.980170e+05</td>\n",
       "      <td>1.980170e+05</td>\n",
       "      <td>198017.000000</td>\n",
       "      <td>1.980170e+05</td>\n",
       "      <td>198017.000000</td>\n",
       "      <td>198017.000000</td>\n",
       "      <td>198017.000000</td>\n",
       "      <td>198017.000000</td>\n",
       "      <td>198017.000000</td>\n",
       "      <td>198017.000000</td>\n",
       "      <td>198017.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.429140</td>\n",
       "      <td>1.607555e-17</td>\n",
       "      <td>3.876790e-16</td>\n",
       "      <td>0.020332</td>\n",
       "      <td>7.311790e-15</td>\n",
       "      <td>0.331451</td>\n",
       "      <td>0.235051</td>\n",
       "      <td>0.206831</td>\n",
       "      <td>0.039593</td>\n",
       "      <td>0.756268</td>\n",
       "      <td>0.168546</td>\n",
       "      <td>0.518011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.494955</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>0.141132</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>0.770043</td>\n",
       "      <td>0.890650</td>\n",
       "      <td>0.671309</td>\n",
       "      <td>0.325001</td>\n",
       "      <td>1.252869</td>\n",
       "      <td>0.559666</td>\n",
       "      <td>1.025491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.873717e+00</td>\n",
       "      <td>-1.436703e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.722663e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.818369e-01</td>\n",
       "      <td>-8.297511e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.091099e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.395544e-01</td>\n",
       "      <td>-2.227995e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.720290e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.954129e-01</td>\n",
       "      <td>9.911036e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.035928e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.071858e+00</td>\n",
       "      <td>3.418910e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.435157e+00</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                male           age        he_uph       he_unitr        he_usg  \\\n",
       "count  198017.000000  1.980170e+05  1.980170e+05  198017.000000  1.980170e+05   \n",
       "mean        0.429140  1.607555e-17  3.876790e-16       0.020332  7.311790e-15   \n",
       "std         0.494955  1.000003e+00  1.000003e+00       0.141132  1.000003e+00   \n",
       "min         0.000000 -1.873717e+00 -1.436703e+00       0.000000 -1.722663e+00   \n",
       "25%         0.000000 -7.818369e-01 -8.297511e-01       0.000000 -1.091099e+00   \n",
       "50%         0.000000 -1.395544e-01 -2.227995e-01       0.000000  1.720290e-01   \n",
       "75%         1.000000  6.954129e-01  9.911036e-01       0.000000  8.035928e-01   \n",
       "max         1.000000  3.071858e+00  3.418910e+00       1.000000  1.435157e+00   \n",
       "\n",
       "             he_upro        he_uglu        he_uket        he_ubil  \\\n",
       "count  198017.000000  198017.000000  198017.000000  198017.000000   \n",
       "mean        0.331451       0.235051       0.206831       0.039593   \n",
       "std         0.770043       0.890650       0.671309       0.325001   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         5.000000       5.000000       5.000000       4.000000   \n",
       "\n",
       "             he_ubld         he_uro      leucocyte  \n",
       "count  198017.000000  198017.000000  198017.000000  \n",
       "mean        0.756268       0.168546       0.518011  \n",
       "std         1.252869       0.559666       1.025491  \n",
       "min         0.000000       0.000000       0.000000  \n",
       "25%         0.000000       0.000000       0.000000  \n",
       "50%         0.000000       0.000000       0.000000  \n",
       "75%         1.000000       0.000000       1.000000  \n",
       "max         5.000000       5.000000       4.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#standardization\n",
    "\n",
    "scaler = StandardScaler()\n",
    "std_cols=['age','he_uph','he_usg']\n",
    "std_df=X_train_features[std_cols]\n",
    "\n",
    "X_train_features[std_cols]=scaler.fit_transform(std_df)\n",
    "X_train_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(y_train)\n",
    "estimate = round(counter[0]/counter[1])\n",
    "step = round((estimate - 1)/3)\n",
    "estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danssa/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/distributed/node.py:151: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 38721 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#https://dask-cuda.readthedocs.io/en/latest/quickstart.html\n",
    "\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Create a Dask Cluster with one worker per GPU\n",
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 0.11.6.post3 of tpot is outdated. Version 0.11.7 was released Wednesday January 06, 2021.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/10100 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9352215917594261\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.6000000000000001, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=2, XGBClassifier__min_child_weight=1, XGBClassifier__n_estimators=750, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.5, XGBClassifier__reg_lambda=6, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9352215917594261\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.6000000000000001, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=2, XGBClassifier__min_child_weight=1, XGBClassifier__n_estimators=750, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.5, XGBClassifier__reg_lambda=6, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9352215917594261\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.6000000000000001, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=2, XGBClassifier__min_child_weight=1, XGBClassifier__n_estimators=750, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.5, XGBClassifier__reg_lambda=6, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9354384466850572\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=16, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.75, XGBClassifier__reg_lambda=4, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=1.0, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9354384466850572\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=16, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.75, XGBClassifier__reg_lambda=4, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=1.0, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9355807118345855\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.6000000000000001, XGBClassifier__gamma=0.6000000000000001, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=15, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.5, XGBClassifier__reg_lambda=6, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=1.0, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9358540021822913\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=16, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0.75, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9359139738072212\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=16, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=1, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9359139738072212\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=16, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=1, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9359139738072212\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=16, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=1, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9359671223491443\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=16, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9359671223491443\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=16, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9359671223491443\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=16, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9359671223491443\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=16, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9359671223491443\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=16, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 51 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 52 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 53 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 54 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 55 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 56 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 57 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 58 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 59 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 60 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 61 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 62 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 63 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 64 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 65 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 66 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 67 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 68 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 69 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 70 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 71 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 72 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 73 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 74 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 75 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 76 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 77 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 78 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 79 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 80 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 81 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 82 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 83 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 84 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 85 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 86 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 87 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 88 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 89 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 90 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 91 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 92 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 93 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 94 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 95 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 96 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 97 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 98 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 99 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 100 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.9360078242002945\tXGBClassifier(input_matrix, XGBClassifier__colsample_bytree=0.8000000000000002, XGBClassifier__gamma=0.0, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=500, XGBClassifier__n_jobs=1, XGBClassifier__objective=binary:logistic, XGBClassifier__reg_alpha=0, XGBClassifier__reg_lambda=8, XGBClassifier__scale_pos_weight=14, XGBClassifier__subsample=0.2, XGBClassifier__tree_method=gpu_hist, XGBClassifier__verbosity=0)\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[06:20:33] /opt/conda/envs/rapids/conda-bld/xgboost_1607619219243/work/src/tree/updater_gpu_hist.cu:786: Exception in gpu_hist: [06:20:33] /opt/conda/envs/rapids/conda-bld/xgboost_1607619219243/work/src/c_api/../data/../common/device_helpers.cuh:400: Memory allocation error on worker 0: std::bad_alloc: CUDA error at: /home/danssa/anaconda3/envs/rapids-0.17/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory\n- Free memory: 8978432\n- Requested memory: 3591368\n\nStack trace:\n  [bt] (0) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(+0x14eb6f) [0x7fae9f7fbb6f]\n  [bt] (1) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(dh::detail::ThrowOOMError(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned long)+0x3ad) [0x7fae9fa354bd]\n  [bt] (2) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(dh::detail::XGBDefaultDeviceAllocatorImpl<xgboost::Entry>::allocate(unsigned long)+0x1df) [0x7fae9fa559df]\n  [bt] (3) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(thrust::detail::vector_base<xgboost::Entry, dh::detail::XGBDefaultDeviceAllocatorImpl<xgboost::Entry> >::vector_base<__gnu_cxx::__normal_iterator<xgboost::Entry const*, std::vector<xgboost::Entry, std::allocator<xgboost::Entry> > > >(__gnu_cxx::__normal_iterator<xgboost::Entry const*, std::vector<xgboost::Entry, std::allocator<xgboost::Entry> > >, __gnu_cxx::__normal_iterator<xgboost::Entry const*, std::vector<xgboost::Entry, std::allocator<xgboost::Entry> > >)+0x5c) [0x7fae9fa55aac]\n  [bt] (4) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(xgboost::common::ProcessBatch(int, xgboost::MetaInfo const&, xgboost::SparsePage const&, unsigned long, unsigned long, xgboost::common::SketchContainer*, int, unsigned long)+0x83) [0x7fae9fa45b73]\n  [bt] (5) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(xgboost::common::DeviceSketch(int, xgboost::DMatrix*, int, unsigned long)+0x752) [0x7fae9fa46982]\n  [bt] (6) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(xgboost::EllpackPageImpl::EllpackPageImpl(xgboost::DMatrix*, xgboost::BatchParam const&)+0x3a9) [0x7fae9faabe49]\n  [bt] (7) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(xgboost::EllpackPage::EllpackPage(xgboost::DMatrix*, xgboost::BatchParam const&)+0x2e) [0x7fae9faac2ee]\n  [bt] (8) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(xgboost::data::SimpleDMatrix::GetEllpackBatches(xgboost::BatchParam const&)+0x9b) [0x7fae9f8a3a7b]\n\n\n\nStack trace:\n  [bt] (0) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(+0x14eb6f) [0x7fae9f7fbb6f]\n  [bt] (1) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(xgboost::tree::GPUHistMakerSpecialised<xgboost::detail::GradientPairInternal<double> >::Update(xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*, xgboost::DMatrix*, std::vector<xgboost::RegTree*, std::allocator<xgboost::RegTree*> > const&)+0x763) [0x7fae9fc26623]\n  [bt] (2) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(xgboost::gbm::GBTree::BoostNewTrees(xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*, xgboost::DMatrix*, int, std::vector<std::unique_ptr<xgboost::RegTree, std::default_delete<xgboost::RegTree> >, std::allocator<std::unique_ptr<xgboost::RegTree, std::default_delete<xgboost::RegTree> > > >*)+0x997) [0x7fae9f8e95e7]\n  [bt] (3) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(xgboost::gbm::GBTree::DoBoost(xgboost::DMatrix*, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*, xgboost::PredictionCacheEntry*)+0x106) [0x7fae9f8ea8a6]\n  [bt] (4) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(+0x25183d) [0x7fae9f8fe83d]\n  [bt] (5) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fae9f802728]\n  [bt] (6) /home/danssa/anaconda3/envs/rapids-0.17/lib/python3.8/lib-dynload/../../libffi.so.7(+0x69ed) [0x7fb3c32929ed]\n  [bt] (7) /home/danssa/anaconda3/envs/rapids-0.17/lib/python3.8/lib-dynload/../../libffi.so.7(+0x6077) [0x7fb3c3292077]\n  [bt] (8) /home/danssa/anaconda3/envs/rapids-0.17/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x2b4) [0x7fb3c32a88b4]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ccd6f40a5d8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mtraining_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mtpot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mtpot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/danssa/proj_ua/progress/CHA+SV*/60model/chasv_60model.v4.1.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/tpot/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[1;32m    771\u001b[0m                     \u001b[0;31m# raise the exception if it's our last attempt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattempts\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/tpot/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_top_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_summary_of_best_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m                     \u001b[0;31m# Delete the temporary cache before exiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cleanup_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/tpot/base.py\u001b[0m in \u001b[0;36m_summary_of_best_pipeline\u001b[0;34m(self, features, target)\u001b[0m\n\u001b[1;32m    888\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitted_pipeline_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbosity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m    907\u001b[0m             eval_group=None, label_transform=label_transform)\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m         self._Booster = train(xgb_options, train_dmatrix,\n\u001b[0m\u001b[1;32m    910\u001b[0m                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_boosting_rounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                               \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mBooster\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \"\"\"\n\u001b[0;32m--> 219\u001b[0;31m     bst = _train_internal(params, dtrain,\n\u001b[0m\u001b[1;32m    220\u001b[0m                           \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                           \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[1;32m   1268\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.17/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \"\"\"\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [06:20:33] /opt/conda/envs/rapids/conda-bld/xgboost_1607619219243/work/src/tree/updater_gpu_hist.cu:786: Exception in gpu_hist: [06:20:33] /opt/conda/envs/rapids/conda-bld/xgboost_1607619219243/work/src/c_api/../data/../common/device_helpers.cuh:400: Memory allocation error on worker 0: std::bad_alloc: CUDA error at: /home/danssa/anaconda3/envs/rapids-0.17/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory\n- Free memory: 8978432\n- Requested memory: 3591368\n\nStack trace:\n  [bt] (0) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(+0x14eb6f) [0x7fae9f7fbb6f]\n  [bt] (1) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(dh::detail::ThrowOOMError(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned long)+0x3ad) [0x7fae9fa354bd]\n  [bt] (2) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(dh::detail::XGBDefaultDeviceAllocatorImpl<xgboost::Entry>::allocate(unsigned long)+0x1df) [0x7fae9fa559df]\n  [bt] (3) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(thrust::detail::vector_base<xgboost::Entry, dh::detail::XGBDefaultDeviceAllocatorImpl<xgboost::Entry> >::vector_base<__gnu_cxx::__normal_iterator<xgboost::Entry const*, std::vector<xgboost::Entry, std::allocator<xgboost::Entry> > > >(__gnu_cxx::__normal_iterator<xgboost::Entry const*, std::vector<xgboost::Entry, std::allocator<xgboost::Entry> > >, __gnu_cxx::__normal_iterator<xgboost::Entry const*, std::vector<xgboost::Entry, std::allocator<xgboost::Entry> > >)+0x5c) [0x7fae9fa55aac]\n  [bt] (4) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(xgboost::common::ProcessBatch(int, xgboost::MetaInfo const&, xgboost::SparsePage const&, unsigned long, unsigned long, xgboost::common::SketchContainer*, int, unsigned long)+0x83) [0x7fae9fa45b73]\n  [bt] (5) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(xgboost::common::DeviceSketch(int, xgboost::DMatrix*, int, unsigned long)+0x752) [0x7fae9fa46982]\n  [bt] (6) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(xgboost::EllpackPageImpl::EllpackPageImpl(xgboost::DMatrix*, xgboost::BatchParam const&)+0x3a9) [0x7fae9faabe49]\n  [bt] (7) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(xgboost::EllpackPage::EllpackPage(xgboost::DMatrix*, xgboost::BatchParam const&)+0x2e) [0x7fae9faac2ee]\n  [bt] (8) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(xgboost::data::SimpleDMatrix::GetEllpackBatches(xgboost::BatchParam const&)+0x9b) [0x7fae9f8a3a7b]\n\n\n\nStack trace:\n  [bt] (0) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(+0x14eb6f) [0x7fae9f7fbb6f]\n  [bt] (1) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(xgboost::tree::GPUHistMakerSpecialised<xgboost::detail::GradientPairInternal<double> >::Update(xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*, xgboost::DMatrix*, std::vector<xgboost::RegTree*, std::allocator<xgboost::RegTree*> > const&)+0x763) [0x7fae9fc26623]\n  [bt] (2) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(xgboost::gbm::GBTree::BoostNewTrees(xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*, xgboost::DMatrix*, int, std::vector<std::unique_ptr<xgboost::RegTree, std::default_delete<xgboost::RegTree> >, std::allocator<std::unique_ptr<xgboost::RegTree, std::default_delete<xgboost::RegTree> > > >*)+0x997) [0x7fae9f8e95e7]\n  [bt] (3) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(xgboost::gbm::GBTree::DoBoost(xgboost::DMatrix*, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*, xgboost::PredictionCacheEntry*)+0x106) [0x7fae9f8ea8a6]\n  [bt] (4) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(+0x25183d) [0x7fae9f8fe83d]\n  [bt] (5) /home/danssa/anaconda3/envs/rapids-0.17/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fae9f802728]\n  [bt] (6) /home/danssa/anaconda3/envs/rapids-0.17/lib/python3.8/lib-dynload/../../libffi.so.7(+0x69ed) [0x7fb3c32929ed]\n  [bt] (7) /home/danssa/anaconda3/envs/rapids-0.17/lib/python3.8/lib-dynload/../../libffi.so.7(+0x6077) [0x7fb3c3292077]\n  [bt] (8) /home/danssa/anaconda3/envs/rapids-0.17/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x2b4) [0x7fb3c32a88b4]\n\n"
     ]
    }
   ],
   "source": [
    "classifier_config_dict = {\n",
    "\n",
    "    # xgboost tree method = gpu hist\n",
    "    \n",
    "    'xgboost.XGBClassifier': {\n",
    "        'n_estimators': [100, 250 ,500, 750, 1000],\n",
    "        'learning_rate': [1e-2, 1e-1, 0.3],\n",
    "        'max_depth': range(2, 11),\n",
    "        'min_child_weight': range(1, 21),\n",
    "        'gamma':np.arange(0, 2.01, 0.2),\n",
    "        'subsample': np.arange(0.2, 1.01, 0.2),\n",
    "        'colsample_bytree': np.arange(0.4,1.01,0.2),\n",
    "        \"reg_alpha\": [0, 0.25, 0.5, 0.75, 1],\n",
    "        \"reg_lambda\": [1, 2, 4, 6, 8],\n",
    "        'scale_pos_weight': [estimate],\n",
    "        'objective': ['binary:logistic'],\n",
    "        'tree_method' : ['gpu_hist'],\n",
    "        'n_jobs': [1],\n",
    "        'verbosity': [0]\n",
    "    },\n",
    "\n",
    "}\n",
    "\n",
    "tpot = TPOTClassifier(scoring=\"roc_auc\",\n",
    "                      cv=5,\n",
    "                      random_state=SEED,\n",
    "                      n_jobs=4,\n",
    "                      verbosity=3,\n",
    "                      generations=100,\n",
    "                      population_size=100,\n",
    "                      use_dask=True,\n",
    "                      warm_start=False,\n",
    "                      config_dict=classifier_config_dict,\n",
    "                      template='Classifier')\n",
    "\n",
    "training_features=X_train_features.copy(deep=True)\n",
    "tpot.fit(training_features, y_train)\n",
    "\n",
    "tpot.export('/home/danssa/proj_ua/progress/CHA+SV*/60model/chasv_60model.v4.1.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.export('/home/danssa/proj_ua/progress/CHA+SV*/60model/chasv_60model.v4.1.py')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi_gpu",
   "language": "python",
   "name": "rapids-0.17"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
